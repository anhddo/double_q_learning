\documentclass{article}
\usepackage{amsfonts}
\usepackage[]{graphicx}
\usepackage[]{amsmath} 
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage[]{hyperref}
%\usepackage[nonatbib]{neurips_2020}
\usepackage[linesnumbered]{algorithm2e}
\usepackage[document]{ragged2e}
\usepackage[backend=biber, style=apa]{biblatex}
\usepackage{caption}
\usepackage{subcaption}
\addbibresource{refs.bib}
%\renewcommand{\familydefault}{\sfdefault}
\newcommand{\incfig}[2]{%
    \def\svgwidth{#1\columnwidth}
    \import{./img/}{#2.pdf_tex}
}
\newcommand{\mbP}{\mathbb{P}}
\newcommand{\mbV}{\mathbb{V}}
\newcommand{\mbE}{\mathbb{E}}
\newcommand{\mcF}{\mathcal{F}}
\DeclareMathOperator*{\argmax}{argmax} 
\DeclareMathOperator*{\argmin}{argmin} 
\DeclareMathOperator*{\clim}{{C--lim}} 
\renewcommand{\algorithmautorefname}{Algorithm}


\author{%
  Anh Do \\
  \texttt{anhddo93@gmail.com} \\
}
\date{}
\title{Research note}
\begin{document}
\maketitle
\tableofcontents
\justify
\section{Inequality.}
\textbf{Cauchy Schawrtz}\\
$|\langle u,v \rangle|^2 \leq \langle u,u \rangle . \langle v,v \rangle$\\
\textbf{Hoeffding Inequality}\\
Let $\bar{X}=\frac{1}{n}\left(X_1 + \cdots X_n\right)$ and $X_i$ is strictly bound by 
$[a_i, b_i]$:\\
$\mathbb{P} \left(\bar{X}-\mathbb{E}(\bar{X}) \geq t\right) 
\leq \exp\left(-\frac{2n^2t^2}{\sum_{i=1}^{n} (a_i - b_i)^2}\right)$
\section{Linear algebra.}
Notation: $||x||_A = \sqrt{x^{t}Ax}$
\subsection{Basic.}
\begin{enumerate}
    \item \textbf{Vector space}\\
        A vector space V is a set that is closed under finite vector addition 
        and scalar multiplication and satisfy eight axioms
    \item \textbf{Linear map}\\
        Let V and W is two vector space. A function $f: V \rightarrow W$ is a \textbf{linear map} if for any $u,v \in V$, we have:
        $f(u + v)=f(u) + f(v)$ and $f(cu)=cf(u)$
    \item \textbf{Linear subspace}\\
        If V is a vector space over a field K and if W is a subset of V, then W is a subspace 
        of V if under the operations of V, W is a vector space over K.
    \item \textbf{Kernel}\\
        Let $L: V \rightarrow W$ is a linear map, then $ker(L)=\{v \in V| L(v)=0\}$
\end{enumerate}
\section{Set Theory.}
\subsection{Function.}
%\begin{figure}[ht]
%        \centering
%        \includegraphics[width=0.2\textwidth]{image/function}
%        \includegraphics[width=0.2\textwidth]{image/not_a_function}
%        \label{fig:function}
%        \caption{Function(left) vs not a function (right)}
%\end{figure}

\section{Probability theory.}
\subsection{Measure theory.}
\begin{enumerate}
    \item \textbf{Atom}\\
        A measurable space $(\Omega, \mathcal{F})$ and a measure function 
        $\mu$, subset $A $ is called an atom when $ A \in \mathcal{F}, \mu(A)>0$ and any subset
        $B \subset A$ will have measure $\mu(B)=0$
    \item \textbf{$\sigma$-algebra}: $\mcF \in 2^{\Omega}$.
        \begin{enumerate}
            \item $\Omega \in \mcF$
            \item $A_i \in \mcF \Rightarrow A_i^{c} \in \mcF$
            \item $A_i \in \mcF \Rightarrow \cup_i A_i \in \mcF$
        \end{enumerate}
    \item \textbf{Measurable space:} $(\mathcal{X}, \mcF)$.\\
            Set $\mathcal{X}$ and $\mcF$ is a $\sigma$-algebra on X. Then $(\mathcal{X}, \mcF)$ is a measurable space.
    \item \textbf{Probability measure function: $\mbP$}.
        \begin{itemize}
            \item $\mbP(\omega) \in [0, 1]$ and $\mbP(\emptyset)=0$ and $\mbP(\Omega)=1$
            \item $\mbP(\cup_i E_i)=\sum_{i}^{}\mbP(E_i), \forall E_i$ disjoint
        \end{itemize}
    \item \textbf{Probability space:} the tripplet $(\Omega, \mcF, \mbP)$
    \item \textbf{Measurable function:} $f:X \rightarrow Y $ where $(X,\mcF) $ and $ (Y, \mathcal{G})$ are two measurable spaces.
        \[f^{-1}(E)=\{x \in X| f(x) \in E\} \in \mcF, \forall E \in \mathcal{G}\]
    \item \textbf{Measurable map}.\\
        Let $(\Omega, \mathcal{F})$ is a measurable space and $\mathcal{G} \subseteq \mathcal{X}$. A function $X: \Omega \rightarrow \mathcal{X}$ is called a measurable map 
        $\mathcal{F}/\mathcal{G}$ if $\{X^{-1}(A): A \in \mathcal{G}\} \in \mathcal{F}$

    \item \textbf{Random variable}.\\
        The r.v X is the "push-forward" of the measure $\mbP$ on $\Omega$ to the "probability density function" $\mathbb{P}_X$ of $\mathbb{R}$.\\
        $(\Omega, \mcF, \mbP)$ is the probability space and $(E, \mathcal{E})$ is a measurable space.
        R.v $X: \Omega \rightarrow E$ is a measurable function with: $X^{-1}(B)=\{\omega: X(\omega) \in B\} \in \mcF, \forall B \in \mathcal{E}$. 
        Any preimage of subset $B \in \mathcal{E}$ is measurable.
    \item \textbf{$\sigma$-algebra generated by X}.\\
        $X:\Omega \rightarrow \mathcal{X}$ between two measurable spaces $(\Omega, \mcF)$ and $(\mathcal{X}, \mathcal{G})$
        then:\\
        $\sigma(X)=\{X^{-1}(A):A \in \mathcal{G}\}$ is the  $\sigma$-algebra generated by X. 
\end{enumerate}
\subsection{Filtration}
\begin{enumerate}
    \item \textbf{Sub $\sigma$-algebra.}\\
        Let $\mathcal{F}$ is a $\sigma$-algebra. If $\sigma$-algebra $\mathcal{G}$ and $\mathcal{G} \subseteq \mathcal{F}$ is sub $\sigma$-algebra.
        
    \item \textbf{Filtration}\\
        Let $X_1, \cdots, X_n$ is a collection of random variable on common measurable space $(\Omega, \mathcal{F})$. A index set I with total 
        order $\leq$, for every $i \in I$, let $\mathcal{F}_t=\sigma(X_{1:t})$ is a sub $\sigma$-algebra of $\mathcal{F}$:
        \[\mathbb{F}:=(\mathcal{F}_t)_{t \in I}\]
        is a filtration, if $\mathcal{F}_k \subseteq \mathcal{F}_l \subseteq \mathcal{A}, \forall k \leq l$. $(\Omega, \mathcal{A}, \mathbb{F}, \mathbb{P})$ is called filtered probability space.\\
        \textbf{Example:}
        Toss a fair coin two times, so the events space $\Omega=\{HT, HH, TH, TT\}$.\\
        $\mathcal{F}_0=\{\emptyset, \Omega\}$\\
        $\mathcal{F}_1=\sigma(X_1)=\{\emptyset, \{HT, HH\}, \{TH, TT\}, \Omega\}$\\
        $\mathcal{F}_2=\sigma(X_1, X_2)=2^{\Omega}$
\end{enumerate}
\section{Subgaussian}
\begin{enumerate}
    \item If $\sigma$-subgaussian then for all $\epsilon \geq 0$:\\
        $\mbP(|X|>\epsilon)\leq exp(-\frac{\epsilon^2}{2\sigma^2})$\\
    \item $\hat{\mu}-\mu$ is $\frac{\sigma}{\sqrt{n}}$-subgaussian: \\
        $\mbP\left(|\hat{\mu}-\mu|>\epsilon\right)<exp\left(-\frac{n\epsilon^2}{2\sigma^2}\right)$
\end{enumerate}
\section{Probability.}
Proof: $\mbV(\bar{X})=\frac{\sigma^2}{n}$
\begin{align*}
    n^2\mbE(\bar{ X })&=\mbE[(X_1+\cdots X_n)(X_1+\cdots X_n)]=n\mbE(X_i^2) + n(n-1)\mu^2 \\
                      &=n(\sigma^2 + \mu^2) - n^2\mu^2-n\mu^2=n\sigma^2-n^2\mu^2
\end{align*}
$\mbE(\bar{X})=\mu$
$\Rightarrow \mbV(\bar{X})=\mbE(\bar{X}^2)+\mbE(\bar{X})^2=\frac{\sigma^2}{n}$
\section{Upper confidence bound.}


%\section{Optimistic policy iteration.}
%\begin{figure}[ht]
%        \centering
%        \incfig{0.6}{drawing}
%        \label{fig:draw}
%        \caption{Two stages finite-horizon MDP}
%\end{figure}
%Let $\hat{r}= \bar{r} + bonus$\\
%$\pi_k(s)=\argmax_{a}\widehat{Q}^{\pi_k}(s, a) $
%\begin{align*}
%\widehat{Q}^{\pi_{k-1}}_1(s_1, \pi_k) \geq \widehat{Q}^{\pi_{k-1}}_1(s_1, \pi_\star) 
%\Rightarrow \hat{r}^{k-1}(s_1, \pi_{k}) + \widehat{P}_{s_1, \pi_{k}}\widehat{V}^{k-1}_2
%\geq  \hat{r}^{k-1}(s_1, \pi_{\star}) + \widehat{P}_{s_1, \pi_{\star}}\widehat{V}^{k-1}_2
%\end{align*}
\section{Excercise For Bandit algorithm books.}
\textbf{2.1}\\
\textbf{2.2}\\
\textbf{2.3}\\
\textbf{2.5}\\
\textbf{2.7}: $(\Omega, \mathcal{F}, \mathbb{P})$ and $B \in \mathcal{F}, \mathbb{P}(B)>0$
.Prove $A \mapsto \mathbb{P}(A|B)$ is a probability measure over $(\Omega, \mathcal{F})$
\textbf{Answer:}\\

We have $\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}$. 
By definition of probability measure, three property need to satisfied:
\begin{itemize}
\item  $\mathbb{P}(\emptyset|B)=\frac{\mathbb{P}(\emptyset \cap B)}{\mathbb{P}(B)}=0$
\item $\mathbb{P}(\Omega|B)=\frac{\mathbb{P}(\Omega \cap B)}{\mathbb{P}(B)}=1$
\item Let $A_1 \in \mathcal{F}, A_2 \in \mathcal{F}$ are 2 disjoint set.
    \[
    \mathbb{P}(A_1 \cup A_2|B)=\frac{\mathbb{P}((A_1 \cup A_2)\cap B)}{\mathbb{P}(B)}
    = \frac{\mathbb{P}(A_1 \cup B) + \mathbb{P}(A_2 \cup B)}{\mathbb{P}(B)}
    = \mathbb{P}(A_1|B) + \mathbb{P}(A_2|B) \; \square
    \]
\end{itemize}
\section{Puterman 1994.}
\textbf{Cesaro limit}\\
\[C-\lim_{N \rightarrow \infty} A_n = \lim_{N \rightarrow \infty} \frac{1}{N}\sum_{n=0}^{N-1}A_n\]
\textbf{Eigenvalues and eigenvectors}.\\
$(Q-v\mathbf{I})$
\subsection{Classification of state}
\begin{enumerate}
    \item Accessible
        \item communicate
    \item Closed set
    \item Irreducible
\end{enumerate}
\subsection{MDP classification}
\begin{enumerate}
    \item Weakly communicating.
\end{enumerate}
\subsection{MDP.}
\begin{enumerate}
    \item \textbf{Diameter} of MDP M\\
$D(M)=\max _{s \neq s^{\prime}} \min _{\pi \in \Pi_{\mathrm{DM}}} \mathbb{E}^{\pi}\left[\min \left\{t \geq 1: S_{t}=s\right\} | S_{1}=s^{\prime}\right]-1$
\end{enumerate}
\section{Markov chain}
\subsection{Limit matrix} 
Let $P^{*}$ is a limit matrix of $P$, \\
\[P^{*}=\clim_{n \rightarrow \infty} P^n=\lim_{N \rightarrow \infty} \frac{1}{N} \sum_{n=0}^{N-1}P^n\]
\\
Properties:
$PP^{*}=P^{*}P=P^{*}P^{*}=P^{*}\Rightarrow (I-P)P^{*}=0$

\subsection{The gain}
\[g(s) = \lim_{N \rightarrow \infty}\frac{1}{N}E_s\left\{\sum_{n=1}^{N}r(s_n)\right\}
=\lim_{N \rightarrow \infty}\frac{1}{N}\sum_{n=1}^{N}P^{n-1}r(s)=P^{*}r(s)\]

\subsection{The bias:}
Let $B^{\#}$ is \textbf{Drazin inverse} of matrix B: $BB^{\#}B=B$ and $B^{\#}B=BB^{\#}$

Drazin inverse $H_P$ of matrix $\mathbf{I} - P$:
\[ H_{P}\equiv(I-P)^{\#}=\left(I-P+P^{*}\right)^{-1}\left(I-P^{*}\right) \]

We call $Z_P=(I-P+P^{*})^{-1}$ is the \textbf{fundamental matrix} and $H_P$ is the \textbf{deviation matrix}.
We have the following properties:
\begin{align*}
    H_P &= Z_P - P^{*}\\
    Z_PP^{*}&=P^{*}
\end{align*}
Next properties relies on Lamond and Puterman (1989):
\[
    H_P=\clim_{N\rightarrow \infty} \sum_{n=1}^{N}(P^n - P^{*})
    =\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{k=0}^{N-1} \sum_{i=0}^{k}\left(P^{k}-P^{*}\right)
\]

Define $v_{N+1}=\sum_{t=1}^{N} P^{t-1} r$ then we have:
\begin{align*}
H_Pr&=\sum_{t=1}^{N}(P^{t}-P^{*})+ \clim_{N \rightarrow \infty}\sum_{t=N+1}^{\infty}(P^{t}-P^{*}) \\
h&=v_{N+1} - Ng + \clim_{N \rightarrow \infty}\sum_{t=N+1}^{\infty}(P^{t}-P^{*})\\
v_{N+1}&=h+Ng+O(1)
\end{align*}

The system will go in steady states then the third term $O(1)$ will be close to zero.
For the discounted MDP:
\begin{align*}
    v_\lambda&=\sum_{t=0}^{\infty}\lambda^{t}P^{t}r= r + \sum_{t=1}^{\infty}\lambda^{t}P^{t}r
=r + \lambda P \sum_{t=0}^{\infty}\lambda^{t}P^{t}r= r + \lambda P v_\lambda  \\
    v_{\lambda}&=(I-\lambda P)^{-1} r=(1+\rho)(\rho I+[I-P])^{-1} r
\end{align*}
Where $(\rho I+[I-P])^{-1}$ is the resolvent of $(I-P)$
\\
In theorem \textbf{(8.2.6)}, by equation: $(I-P)g=0$ \textbf{(8.2.11)} g is an element in the  null space of $(I-P)$ then
add the second equation: $g+(I-P) h=r$ \textbf{(8.2.12)} we uniquely determine the value of $g=P^{*}r$

\subsection{Irreducible}
\subsection{Periocity}
\section{Meeting.}
$V_t$ dont change much. Each term is bound but exponetial in D
\section{Paper summary.}
\subsection{UCRL. Auer 2007}
The UCRL algorithm is a classic reinforcement learning algorithm for ergodic MDP. It brings the
idea of Upper Confidence Bound algorithm for multi-armed bandit problems to solve the MDP. Basically, it constructs a upper bound for it
true reward and true transition dynamic so the true quantity is in the confidence interval with
high probability. By doing that, we have an efficient algorithm for balancing the exploration and
exploitation. Moreover, UCB is an algorithm that can be proved to be efficient sampling.
\subsection{REGAL, Parlett, 2009}
\subsection{UCRL2}
\subsection{Is Qlearning Provably efficient? Jin, et al 2019}

\section{Linear Approximation Reinforcement learning.}
\subsection{Optimistic policy iteration}
\begin{algorithm}[H]
\SetAlgoLined
Initialize $V_a\leftarrow \epsilon I_n$ \\
\For{each episodes}{
    \For{$h : 1 \leftarrow H$}{
    $a_h \leftarrow \argmax_a\left( \phi(x_h)^T w_a +\sqrt{\phi(x_h)^T V_a^{-1}\phi(x_h)} \right)$ \\
    Observe $x_{h+1}$ from environment\\
    Append $<x_h, a_h, r_h, x_{h+1}>$ in D \\
    $V_{a_h}\leftarrow V_{a_h} + \phi(x_h)\phi(x_h)^T$
    }
    $w_a' \leftarrow w_a$\\
    $S_a\leftarrow 0$\\
    \For{each $<x_h, a_h, r_h, x_{h+1}>$ in D}{
        $a_{h+1} \leftarrow argmax_a\left( \phi(x_{h+1})^T w'_a + \sqrt{\phi(x_{h+1})^T V_a^{-1}\phi(x_{h+1})}\right)$ \\
        $Q(x_{h}, a_{h}) = r_h + \phi(x_{h+1})^T w_{a_{h+1}} + \sqrt{\phi(x_{h+1})^T V_{a_{h+1}}^{-1}\phi(x_{h+1})}$ \\
        $S_a \leftarrow S_a + \phi(x_h)^T Q(x_{h}, a_{h})$
    }
    $w_a\leftarrow V_a^{-1}S_a$
}
\caption{Least square optimistic policy iteration}
\end{algorithm}

In \autoref{algo:pol_iter}, line 2 to 6 defines a new policy that is greedy w.r.t the previous 
estimate state value (policy improvement). In line 9 to 12, the model parameter $\mathbf{w}_k$ is 
iteratively updated with the observed data (policy evaluation).
\begin{algorithm}[H]
\SetAlgoLined
\For{$t= 1 \rightarrow T$}{
    \For{$h = 1 \rightarrow H$}{
        $t \leftarrow t+1$\\
        Take action $a_t \leftarrow \argmax_a\left( \phi(s_t, a)^T w_{k-1} +\beta\sqrt{\phi(s_t, a)^T M_{k-1}^{-1}\phi(s_t, a)} \right)$ \\
        Observe $s_{t+1}$\\
    }
    $M_k \leftarrow \sum_{i=1}^{t}\phi(s_i, a_i^k)\phi(s_i, a_i^k)^T + \lambda \mathbf{I}$\\
    $a_{i + 1}^k \leftarrow \argmax_a\left( \phi(s_{i+1}, a)^T \mathbf{w}_{k-1} +\beta\sqrt{\phi(s_{i+1}, a)^T M_{k-1}^{-1}\phi(s_{i+1}, a)} \right)$ \\
    \For{$n=1 \rightarrow N$}{
        $Q_k(.,.) \leftarrow \phi^T \mathbf{w}_{k} +\beta\sqrt{\phi^T M_{k}^{-1}\phi}$ \\
        $\mathbf{w}_k \leftarrow M_{k}^{-1}\sum_{i=1}^{t}\phi(s_i, a_{i}^k) \left[ r(s_i, a_{i}^k) + \gamma Q_k(s_{i+1}, a_{i+1}^k)\right]$ \\
    }
    $k \leftarrow k+1$
}
\caption{Optimistic policy iteration algorithm}
\label{algo:pol_iter}
\end{algorithm}



\subsection{Optimistic value iteration for average reward problem.}
\begin{algorithm}[H]
\SetAlgoLined
\For{t=1,...,T}{
    Take action $a_t \leftarrow \argmax_a Q(s_t, a)$\\
    Observe $s_{t+1}$\\
    $M_t \leftarrow \sum_{k=1}^{t}\phi(s_k, a_i)\phi(s_k, a_k)^T + \lambda \mathbf{I}$\\
    $Q_{t-1}(s_{t+1}, a) \leftarrow \phi(s_{t+1}, a)^T \mathbf{w}_{t-1} + \beta
    \sqrt{\phi(s_{t+1}, a)^T M_{t-1}^{-1}\phi(s_{t+1}, a)}$\\
    $V_{t-1}(s_{t+1}) \leftarrow \min \{\argmax_a(Q_{t-1}(s_{t+1}, a), H \}$\\
    $\mathbf{w}_t \leftarrow M_t^{-1}\sum_{k=1}^{t}\phi(s_k, a_k) [ r(s_k, a_k) + \gamma V_{t-1}(s_{k+1}) ]$ 
}
\caption{Optimistic value iteration for average reward problem}
\label{algo:val_iter}
\end{algorithm}

\subsection{Experiment}
In this section, we apply the value iteration algorithm on several classical control problems \autocite{gym}.
To extract the feature of the given observation, we use multivariate Fourier basis features \autocite{fourier} of order 2.
The experimental result is shown in \autoref{fig:experiment}

\textbf{Acrobot}
\autocite{acrobot}
The acrobot is a system with two links pendulum, where only the second joint 
between the two links is actuated. Initially, the links are hanging 
downwards, and the goal is to swing the end of the lower link up to a given height.
The environment has six observations, consists of the sine and cosine of the two rotational joint
angles and the joint angular velocities. The action is either applying +1, 0 or -1 torque on the joint between
the two pendulum links. 

\textbf{CartPole} \autocite{cartpole} 
In the Cartpole environment, a pole is attached by an unactuated joint to a cart,
which moves along a frictionless rail. The system is controlled by pushing 
the cart to the left or right.  The observation consists of the position 
and velocity of the cart, pole angle, and pole velocity at the tip.
The pendulum starts upright, and the goal is to prevent it from falling over. 

%\begin{figure}[ht]
%    \centering
%    \begin{subfigure}[b]{0.45\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{image/Cartpole-v0-avg-reward}
%        \caption{Cartpole}
%        \label{fig:cartpole}
%    \end{subfigure}
%    \hfill
%    \begin{subfigure}[b]{0.45\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{image/acrobot-v1}
%        \caption{Acrobot}
%        \label{fig:acrobot}
%    \end{subfigure}
%    \caption{Experimental result for \autoref{algo:pol_iter} (Policy iteration) and \autoref{algo:val_iter} (Value iteration) }
%    \label{fig:experiment}
%\end{figure}

\printbibliography
\end{document}
