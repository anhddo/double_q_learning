\subsection{Markov decision process}
For ease of exposition, we restrict our attention to countable MDPs and the discounted total
expected reward criterion. However, under some technical conditions, the results extend to
continuous state-action MDPs, too. This also holds true for the results presented in later
parts of this book. A countable MDP is defined as a triplet $\mathcal{M}=\left(\mathcal{X}, \mathcal{A}, \mathcal{P}_{0}\right),$ where $\mathcal{X}$ is the countable non-
empty set of states, $\mathcal{A}$ is the countable non-empty set of actions. The transition probability kernel $\mathcal{P}_{0}$ assigns to each state-action pair $(x, a) \in \mathcal{X} \times \mathcal{A}$ a probability measure over $\mathcal{X} \times \mathbb{R},$ which we shall denote by $\mathcal{P}_{0}(\cdot \mid x, a) .$ The semantics of $\mathcal{P}_{0}$ is the following: For $U \subset \mathcal{X} \times \mathbb{R}$ $\mathcal{P}_{0}(U \mid x, a)$ gives the probability that the next state and the associated reward belongs to the set $U$ provided that the current state is $x$ and the action taken is $a$. factor $0 \leq \gamma \leq 1$ whose role will become clear soon.

The transition probability kernel gives rise to the state transition probability kernel, $\mathcal{P},$ which, for any $(x, a, y) \in \mathcal{X} \times \mathcal{A} \times \mathcal{X}$ triplet gives the probability of moving from state $x$ to some other state $y$ provided that action $a$ was chosen in state $x$ :
$$
\mathcal{P}(x, a, y)=\mathcal{P}_{0}(\{y\} \times \mathbb{R} \mid x, a)
$$
In addition to $\mathcal{P}, \mathcal{P}_{0}$ also gives rise to the immediate reward function $r: \mathcal{X} \times \mathcal{A} \rightarrow \mathbb{R}$
which gives the expected immediate reward received when action $a$ is chosen in state $x$ : If $\left(Y_{(x, a)}, R_{(x, a)}\right) \sim \mathcal{P}_{0}(\cdot \mid x, a),$ then
$$
r(x, a)=\mathbb{E}\left[R_{(x, a)}\right]
$$
In what follows, we shall assume that the rewards are bounded by some quantity $\mathcal{R}>0$ for any $(x, a) \in \mathcal{X} \times \mathcal{A},\left|R_{(x, a)}\right| \leq \mathcal{R}$ almost surely $\left._{ }^{\beta}\right] \quad$ It is immediate that if the random rewards are bounded by $\mathcal{R}$ then $\|r\|_{\infty}=\sup _{(x, a) \in \mathcal{X} \times \mathcal{A}}|r(x, a)| \leq \mathcal{R}$ also holds. An MDP is
called finite if both $\mathcal{X}$ and $\mathcal{A}$ are finite.
Markov Decision Processes are a tool for modeling sequential decision-making problems where a decision maker interacts with a system in a sequential fashion. Given an MDP $\mathcal{M}$
this interaction happens as follows: Let $t \in \mathbb{N}$ denote the current time (or stage), let $X_{t} \in \mathcal{X}$
and $A_{t} \in \mathcal{A}$ denote the random state of the system and the action chosen by the decision
maker at time $t,$ respectively. Once the action is selected, it is sent to the system, which
makes a transition:
$$
\left(X_{t+1}, R_{t+1}\right) \sim \mathcal{P}_{0}\left(\cdot \mid X_{t}, A_{t}\right)
$$
In particular, $X_{t+1}$ is random and $\mathbb{P}\left(X_{t+1}=y \mid X_{t}=x, A_{t}=a\right)=\mathcal{P}(x, a, y)$ holds for any $x, y \in \mathcal{X}, a \in \mathcal{A} .$ Further, $\mathbb{E}\left[R_{t+1} \mid X_{t}, A_{t}\right]=r\left(X_{t}, A_{t}\right) .$ The decision maker then observes
the next state $X_{t+1}$ and reward $R_{t+1},$ chooses a new action $A_{t+1} \in \mathcal{A}$ and the process is
repeated. The goal of the decision maker is to come up with a way of choosing the actions
so as to maximize the expected total discounted reward.

The decision maker can select its actions at any stage based on the observed history. A rule describing the way the actions are selected is called a behavior. A behavior of the decision
maker and some initial random state $X_{0}$ together define a random state-action-reward sequence $\left(\left(X_{t}, A_{t}, R_{t+1}\right) ; t \geq 0\right),$ where $\left(X_{t+1}, R_{t+1}\right)$ is connected to $\left(X_{t}, A_{t}\right)$ by $(\mathbb{1})$ and $A_{t}$ is the action prescribed by the behavior based on the history $\left.X_{0}, A_{0}, R_{1}, \ldots, X_{t-1}, A_{t-1}, R_{t}, X_{t}\right]$ The return underlying a behavior is defined as the total discounted sum of the rewards
incurred:
$$
\mathcal{R}=\sum_{t=0}^{\infty} \gamma^{t} R_{t+1}
$$




\subsection{Value iteration}
The obvious way of finding an optimal behavior in some MDP is to list all behaviors and
then identify the ones that give the highest possible value for each initial state. Since, in
general, there are too many behaviors, this plan is not viable. A better approach is based on
computing value functions. In this approach, one first computes the so-called optimal value
function, which then allows one to determine an optimal behavior with relative easiness.
The optimal value, $V^{*}(x),$ of state $x \in \mathcal{X}$ gives the highest achievable expected return when
the process is started from state $x .$ The function $V^{*}: \mathcal{X} \rightarrow \mathbb{R}$ is called the optimal value
function. A behavior that achieves the optimal values in all states is optimal.
Deterministic stationary policies represent a special class of behaviors, which, as we shall see
soon, play an important role in the theory of MDPs. They are specified by some mapping $\pi,$ which maps states to actions (i.e., $\pi: \mathcal{X} \rightarrow \mathcal{A}) .$ Following $\pi$ means that at any time $t \geq 0$
the action $A_{t}$ is selected using
$$
A_{t}=\pi\left(X_{t}\right)
$$
More generally, a stochastic stationary policy (or just stationary policy) $\pi$ maps states to
distributions over the action space. When referring to such a policy $\pi,$ we shall use $\pi(a \mid x)$
to denote the probability of action $a$ being selected by $\pi$ in state $x .$ Note that if a stationary policy is followed in an MDP, i.e., if
$$
A_{t} \sim \pi\left(\cdot \mid X_{t}\right), \quad t \in \mathbb{N}
$$

the state process $\left(X_{t} ; t \geq 0\right)$ will be a (time-homogeneous) Markov chain. We will use $\Pi_{\text {stat }}$
to denote the set of all stationary policies. For brevity, in what follows, we will often say
just "policy" instead of "stationary policy", hoping that this will not cause confusion.

A stationary policy and an MDP induce what is called a Markov reward processes $(\mathrm{MRP}):$ An
$\mathrm{MRP}$ is determined by the pair $\mathcal{M}=\left(\mathcal{X}, \mathcal{P}_{0}\right),$ where now $\mathcal{P}_{0}$ assigns a probability measure
over $\mathcal{X} \times \mathbb{R}$ to each state. An MRP $\mathcal{M}$ gives rise to the stochastic process $\left(\left(X_{t}, R_{t+1}\right) ; t \geq 0\right)$ where $\left(X_{t+1}, R_{t+1}\right) \sim \mathcal{P}_{0}\left(\cdot \mid X_{t}\right) .$ (Note that $\left(Z_{t} ; t \geq 0\right), Z_{t}=\left(X_{t}, R_{t}\right)$ is a time-homogeneous
Markov process, where $R_{0}$ is an arbitrary random variable, while $\left(\left(X_{t}, R_{t+1}\right) ; t \geq 0\right)$ is a
second-order Markov process.) Given a stationary policy $\pi$ and the $\mathrm{MDP} \mathcal{M}=\left(\mathcal{X}, \mathcal{A}, \mathcal{P}_{0}\right)$
the transition kernel of the MRP $\left(\mathcal{X}, \mathcal{P}_{0}^{\pi}\right)$ induced by $\pi$ and $\mathcal{M}$ is defined using $\mathcal{P}_{0}^{\pi}(\cdot \mid x)=$ $\sum_{a \in \mathcal{A}} \pi(a \mid x) \mathcal{P}_{0}(\cdot \mid x, a) .$ An MRP is called finite if its state space is finite. Let us now define value functions underlying stationary policies. $^{6}$ For this, let us fix some
policy $\pi \in \Pi_{\text {stat }} .$ The value function, $V^{\pi}: \mathcal{X} \rightarrow \mathbb{R},$ underlying $\pi$ is defined by
$$
V^{\pi}(x)=\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^{t} R_{t+1} \mid X_{0}=x\right], \quad x \in \mathcal{X}
$$

with the understanding (i) that the process $\left(R_{t} ; t \geq 1\right)$ is the "reward-part" of the process $\left(\left(X_{t}, A_{t}, R_{t+1}\right) ; t \geq 0\right)$ obtained when following policy $\pi$ and $(i i) X_{0}$ is selected at random such that $\mathbb{P}\left(X_{0}=x\right)>0$ holds for all states $x .$ This second condition makes the conditional expectation in (7) well-defined for every state. If the initial state distribution satisfies this condition, it has no influence on the definition of values.
The value function underlying an MRP is defined the same way and is denoted by $V:$
$$
V(x)=\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^{t} R_{t+1} \mid X_{0}=x\right], \quad x \in \mathcal{X}
$$
It will also be useful to define the action-value function, $Q^{\pi}: \mathcal{X} \times \mathcal{A} \rightarrow \mathbb{R},$ underlying a
policy $\pi \in \Pi_{\text {stat }}$ in an MDP: Assume that the first action $A_{0}$ is selected randomly such that
$\mathbb{P}\left(A_{0}=a\right)>0$ holds for all $a \in \mathcal{A},$ while for the subsequent stages of the decision process
the actions are chosen by following policy $\pi .$ Let $\left(\left(X_{t}, A_{t}, R_{t+1}\right) ; t \geq 0\right)$ be the resulting
stochastic process, where $X_{0}$ is as in the definition of $V^{\pi} .$ Then
$$
Q^{\pi}(x, a)=\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^{t} R_{t+1} \mid X_{0}=x, A_{0}=a\right], \quad x \in \mathcal{X}, a \in \mathcal{A}
$$
Similarly to $V^{*}(x),$ the optimal action-value $Q^{*}(x, a)$ at the state-action pair $(x, a)$ is defined
as the maximum of the expected return under the constraints that the process starts at state
$x,$ and the first action chosen is $a .$ The underlying function $Q^{*}: \mathcal{X} \times \mathcal{A} \rightarrow \mathbb{R}$ is called the
optimal action-value function.
The optimal value- and action-value functions are connected by the following equations:
$$
\begin{aligned}
V^{*}(x) &=\sup _{a \in \mathcal{A}} Q^{*}(x, a), \quad x \in \mathcal{X} \\
Q^{*}(x, a) &=r(x, a)+\gamma \sum_{y \in \mathcal{X}} \mathcal{P}(x, a, y) V^{*}(y), \quad x \in \mathcal{X}, a \in \mathcal{A}
\end{aligned}
$$
In the class of MDPs considered here, an optimal stationary policy always exists:
$$
V^{*}(x)=\sup _{\pi \in \Pi_{\text {stat }}} V^{\pi}(x), \quad x \in \mathcal{X}
$$
In fact, any policy $\pi \in \Pi_{\text {stat }}$ which satisfies the equality
$$
\sum_{a \in \mathcal{A}} \pi(a \mid x) Q^{*}(x, a)=V^{*}(x)
$$
simultaneously for all states $x \in \mathcal{X}$ is optimal. Notice that in order (8) to hold, $\pi(\cdot \mid x)$
must be concentrated on the set of actions that maximize $Q^{*}(x, \cdot) .$ In general, given some
action-value function, $Q: \mathcal{X} \times \mathcal{A} \rightarrow \mathbb{R},$ an action that maximizes $Q(x, \cdot)$ for some state $x$ is
called greedy with respect to $Q$ in state $x .$ A policy that chooses greedy actions only with
respect to $Q$ in all states is called greedy w.r.t. $Q$.

Thus, a greedy policy with respect to $Q^{*}$ is optimal, i.e., the knowledge of $Q^{*}$ alone is
sufficient for finding an optimal policy. Similarly, knowing $V^{*}, r$ and $\mathcal{P}$ also suffices to act
 

optimally.
The next question is how to find $V^{*}$ or $Q^{*}$. Let us start with the simpler question of how to find the value function of a policy:

Fact 1 (Bellman Equations for Deterministic Policies): Fix an $M D P \mathcal{M}=\left(\mathcal{X}, \mathcal{A}, \mathcal{P}_{0}\right), a$ discount factor $\gamma$ and deterministic policy $\pi \in \Pi_{\text {stat. }}$ Let r \text { be the immediate reward function } ~
of $\mathcal{M} .$ Then $V^{\pi}$ satisfies
$$
V^{\pi}(x)=r(x, \pi(x))+\gamma \sum_{y \in \mathcal{X}} \mathcal{P}(x, \pi(x), y) V^{\pi}(y), \quad x \in \mathcal{X}
$$
This system of equations is called the Bellman equation for $V^{\pi} .$ Define the Bellman operator underlying $\pi, T^{\pi}: \mathbb{R}^{\mathcal{X}} \rightarrow \mathbb{R}^{\mathcal{X}},$ by
$$
\left(T^{\pi} V\right)(x)=r(x, \pi(x))+\gamma \sum_{v \in \mathcal{X}} \mathcal{P}(x, \pi(x), y) V(y), \quad x \in \mathcal{X}
$$
With the help of $T^{\pi},$ Equation (9) can be written in the compact form
$$
T^{\pi} V^{\pi}=V^{\pi}
$$
Note that this is a linear system of equations in $V^{\pi}$ and $T^{\pi}$ is an affine linear operator. If
$0<\gamma<1$ then $T^{\pi}$ is a maximum-norm contraction and the fixed-point equation $T^{\pi} V=V$
has a unique solution.
When the state space $\mathcal{X}$ is finite, say, it has $D$ states, $\mathbb{R}^{\mathcal{X}}$ can be identified with the $D$ dimensional Euclidean space and $V \in \mathbb{R}^{\mathcal{X}}$ can be thought of as a $D$ -dimensional vector: $V \in$
$\mathbb{R}^{D} .$ With this identification, $T^{\pi} V$ can also be written as $r^{\pi}+\gamma P^{\pi} V$ with an appropriately defined vector $r^{\pi} \in \mathbb{R}^{D}$ and matrix $P^{\pi} \in \mathbb{R}^{D \times D} .$ In this case, (10) can be written in the
form
$$
r^{\pi}+\gamma P^{\pi} V^{\pi}=V^{\pi}
$$
The above facts also hold true in MRPs, where the Bellman operator $T: \mathbb{R}^{\mathcal{X}} \rightarrow \mathbb{R}^{\mathcal{X}}$ is
defined by
$$
(T V)(x)=r(x)+\gamma \sum_{y \in \mathcal{X}} \mathcal{P}(x, y) V(y), \quad x \in \mathcal{X}
$$
The above facts also hold true in MRPs, where the Bellman operator $T: \mathbb{R}^{\mathcal{X}} \rightarrow \mathbb{R}^{\mathcal{X}}$ is
defined by
$$
(T V)(x)=r(x)+\gamma \sum_{y \in \mathcal{X}} \mathcal{P}(x, y) V(y), \quad x \in \mathcal{X}
$$
The optimal value function is known to satisfy a certain fixed-point equation:
Fact 2 (Bellman Optimality Equations): The optimal value function satisfies the fixed-point
equation
$$
V^{*}(x)=\sup _{a \in \mathcal{A}}\left\{r(x, a)+\gamma \sum_{y \in \mathcal{X}} \mathcal{P}(x, a, y) V^{*}(y)\right\}, \quad x \in \mathcal{X}
$$
$$
\left(T^{*} V\right)(x)=\sup _{a \in \mathcal{A}}\left\{r(x, a)+\gamma \sum_{y \in \mathcal{X}} \mathcal{P}(x, a, y) V(y)\right\}, \quad x \in \mathcal{X}
$$
Note that this is a nonlinear operator due to the presence of sup. With the help of $T^{*}$,
Equation (12) can be written compactly as
$$
T^{*} V^{*}=V^{*}
$$
If $0<\gamma<1,$ then $T^{*}$ is a maximum-norm contraction, and the fixed-point equation $T^{*} V=V$
has a unique solution.
In order to minimize clutter, in what follows we will write expressions like $\left(T^{\pi} V\right)(x)$ as $T^{\pi} V(x),$ with the understanding that the application of operator $T^{\pi}$ takes precedence to the applycation of the point evaluation operator, ". $(x) "$. The action-value functions underlying a policy (or an MRP) and the optimal action-value
function also satisfy some fixed-point equations similar to the previous ones:

Fact 3 (Bellman Operators and Fixed-point Equations for Action-value Functions): With a slight abuse of notation, define $T^{\pi}: \mathbb{R}^{\mathcal{X} \times \mathcal{A}} \rightarrow \mathbb{R}^{\mathcal{X} \times \mathcal{A}}$ and $T^{*}: \mathbb{R}^{\mathcal{X} \times \mathcal{A}} \rightarrow \mathbb{R}^{\mathcal{X} \times \mathcal{A}}$ as follows:
$$
\begin{aligned}
T^{\pi} Q(x, a) &=r(x, a)+\gamma \sum_{y \in \mathcal{X}} \mathcal{P}(x, a, y) Q(y, \pi(y)), & &(x, a) \in \mathcal{X} \times \mathcal{A} \\
T^{*} Q(x, a) &=& r(x, a)+\gamma \sum_{y \in \mathcal{X}} \mathcal{P}(x, a, y) \sup _{a^{\prime} \in \mathcal{A}} Q\left(y, a^{\prime}\right), &(x, a) \in \mathcal{X} \times \mathcal{A}
\end{aligned}
$$

\section{Reinforcment learning}
Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.

Basic reinforcement is modeled as a Markov decision process (MDP):

a set of environment and agent states, S;
a set of actions, A, of the agent;
 $P_{a}(s,s')=\Pr(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)$ $ P_{a}(s,s')=\Pr(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)$ is the probability of transition t from state s to state  $s ^{'}$ under action a
$R_{a}(s,s')$  is the immediate reward after transition from s to  $s ^{'}$ with action a.
A reinforcement learning agent interacts with its environment in discrete time steps. At each time t, the agent receives the current state $ s_{t}$ and reward  $r_{t}$. It then chooses an action $a_{t}$ from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state $s_{t+1}$ and the reward $r_{t+1}$ associated with the transition $(s_{t},a_{t},s_{t+1})$ is determined. The goal of a reinforcement learning agent is to learn a policy: $\pi :A\times S\rightarrow [0,1]$, $\pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)$ which maximizes the expected cumulative reward.

Formulating the problem as a MDP assumes the agent directly observes the current environmental state; in this case the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a Partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.

When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.

Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers[3] and Go (AlphaGo).

Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:

A model of the environment is known, but an analytic solution is not available;
Only a simulation model of the environment is given (the subject of simulation-based optimization);[4]
The only way to collect information about the environment is to interact with it.
The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.
