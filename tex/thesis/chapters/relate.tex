Undiscounted MDPs. Most no-regret algorithms for infinite-horizon undiscounted MDPs focus on tabular representations, and apply optimism to a model of the transition dynamics (Bartlett \& Tewari, 2009 , Fruit et al., 2018 , Jaksch et al., 2010 , Jian et al., 2019 , Ouyang et al., 2017 , Talebi \& Maillard,
2018). For a weakly-communicating MDP with $S$ states, $A$ actions, and diameter $D$, these algorithms achieve the minimax lower bound of $\mathcal{O}(\sqrt{D S A T})$ (Jaksch et al., 2010 ) with high probability up to constant factors (typically $\sqrt{D S}$ ). The downside of model-based algorithms is that they are memory intensive in large-scale MDPs, as they require $S^{2} A$ storage, and as well as being difficult to extend to continuous-valued states.

Among model-free algorithms for tabular MDPs, Wei et al., 2019 show that optimistic Q-learning achieves $\mathcal{O}\left(\operatorname{sp}\left(V_{*}\right)(S A)^{1 / 3} T^{2 / 3}\right)$ regret in weakly-communicating MDPs, where $\operatorname{sp}\left(V_{*}\right)$ is the span of the optimal state-value function (upper bounded by the diameter $D$ ). In the case of uniformly ergodic MDPs, they show a bound of $\mathcal{O}\left(\sqrt{t_{\operatorname{mix}}^{3} \rho A T}\right)$ on expected regret, where $t_{\operatorname{mix}}$ is the mixing time and $\rho$ is the stationary distribution mismatch coefficient. Our optimistic value iteration is a non-trivial generalization of their Q-learning approach to a setting with function approximation.
In the model-free setting with function approximation, the Politex algorithm (Abbasi-Yadkori et al., 2019 , a variant of approximate policy iteration, achieves $\mathcal{O}\left(d^{1 / 2} T^{3 / 4}\right),$ where $d$ is the size of the compressed state-action space (i.e. number of features). The AAPI algorithm of Hao et al., 2020 achieves regret $\mathcal{O}\left(T^{2 / 3}\right)$ using a version of POLITEX with an adaptive learning rate. However, these algorithms do not explicitly tackle exploration, and instead make additional assumptions - namely, that the MDP is ergodic, and that all policies are sufficiently exploratory so that the value functions can be estimated with small error from on-policy data.

Episodic MDPs. Optimistic value-based algorithms have been more popular in episodic MDPs. In the tabular case with horizon $H,$ Jin et al., 2018 show an $\mathcal{O}\left(\sqrt{H^{3} S A T}\right)$ regret bound for Q-learning with UCB-style exploration. With function approximation, Cai et al., 2019 , Jin et al., 2019 , L. F. Yang and Wang, 2019 show an $\mathcal{O}\left(\sqrt{d^{3} H^{3} T}\right)$ regret bound for an optimistic version of least-squares value iteration in linear MDPs. The RLSVI algorithm Osband et al., 2016 performs exploration in the value function parameter space, but its worse-case regret bound of $\mathcal{O}\left(\sqrt{H^{5} S^{3} A T}\right)$ holds only in the tabular setting (Russo, 2019 ).

Lemma 2 (Restated). Let $V^{*}$ be the optimal value function in the discounted MDP with discount factor $\gamma$ and $v^{*}$ be the optimal value function in the undiscounted MDP. Then,
1. $\left|J^{*}-(1-\gamma) V^{*}(s)\right| \leq(1-\gamma) \operatorname{sp}\left(v^{*}\right), \forall s \in \mathcal{S}$
2. $\operatorname{sp}\left(V^{*}\right) \leq 2 \operatorname{sp}\left(v^{*}\right)$
Let $\pi^{*}$ and $\pi_{\gamma}$ be the optimal policy under undiscounted and discounted settings, respectively. By Bellman's equation, we have
$$
v^{*}(s)=r\left(s, \pi^{*}(s)\right)-J^{*}+\mathbb{E}_{s^{\prime} \sim p\left(\cdot \mid s, \pi^{*}(s)\right)} v^{*}\left(s^{\prime}\right)
$$
Consider a state sequence $s_{1}, s_{2}, \cdots$ generated by $\pi^{*} .$ Then, by sub-optimality of $\pi^{*}$ for the discounted setting, we have
$$
\begin{aligned}
V^{*}\left(s_{1}\right) & \geq \mathbb{E}\left[\sum_{t=1}^{\infty} \gamma^{t-1} r\left(s_{t}, \pi^{*}\left(s_{t}\right)\right) \mid s_{1}\right] \\
&=\mathbb{E}\left[\sum_{t=1}^{\infty} \gamma^{t-1}\left(J^{*}+v^{*}\left(s_{t}\right)-v^{*}\left(s_{t+1}\right)\right) \mid s_{1}\right] \\
&=\frac{J^{*}}{1-\gamma}+v^{*}\left(s_{1}\right)-\mathbb{E}\left[\sum_{t=2}^{\infty}\left(\gamma^{t-2}-\gamma^{t-1}\right) v^{*}\left(s_{t}\right) \mid s_{1}\right] \\
& \geq \frac{J^{*}}{1-\gamma}+\min _{s} v^{*}(s)-\max _{s} v^{*}(s) \sum_{t=2}^{\infty}\left(\gamma^{t-2}-\gamma^{t-1}\right) \\
&=\frac{J^{*}}{1-\gamma}-\operatorname{sp}\left(v^{*}\right)
\end{aligned}
$$

where the first equality is by the Bellman equation for the undiscounted setting.
Similarly, for the other direction, let $s_{1}, s_{2}, \cdots$ be generated by $\pi_{\gamma}$. We have
$$
\begin{aligned}
V^{*}\left(s_{1}\right) &=\mathbb{E}\left[\sum_{t=1}^{\infty} \gamma^{t-1} r\left(s_{t}, \pi_{\gamma}\left(s_{t}\right)\right) \mid s_{1}\right] \\
& \leq \mathbb{E}\left[\sum_{t=1}^{\infty} \gamma^{t-1}\left(J^{*}+v^{*}\left(s_{t}\right)-v^{*}\left(s_{t+1}\right)\right) \mid s_{1}\right] \\
&=\frac{J^{*}}{1-\gamma}+v^{*}\left(s_{1}\right)-\mathbb{E}\left[\sum_{t=2}^{\infty}\left(\gamma^{t-2}-\gamma^{t-1}\right) v^{*}\left(s_{t}\right) \mid s_{1}\right] \\
& \leq \frac{J^{*}}{1-\gamma}+\max _{s} v^{*}(s)-\min _{s} v^{*}(s) \sum_{t=2}^{\infty}\left(\gamma^{t-2}-\gamma^{t-1}\right) \\
&=\frac{J^{*}}{1-\gamma}+\operatorname{sp}\left(v^{*}\right)
\end{aligned}
$$
where the first inequality is by sub-optimality of $\pi_{\gamma}$ for the undiscounted setting.
2. Using previous part, for any $s_{1}, s_{2} \in \mathcal{S},$ we have
$$
\left|V^{*}\left(s_{1}\right)-V^{*}\left(s_{2}\right)\right| \leq\left|V^{*}\left(s_{1}\right)-\frac{J^{*}}{1-\gamma}\right|+\left|V^{*}\left(s_{2}\right)-\frac{J^{*}}{1-\gamma}\right| \leq 2 \operatorname{sp}\left(v^{*}\right)
$$
Thus, $\operatorname{sp}\left(V^{*}\right) \leq 2 \operatorname{sp}\left(v^{*}\right)$


Lemma 4. With probability at least $1-\delta$,
$$
\sum_{t=1}^{T}\left(Q^{*}\left(s_{t}, a_{t}\right)-\gamma V^{*}\left(s_{t}\right)-r\left(s_{t}, a_{t}\right)\right) \leq 2 \operatorname{sp}\left(v^{*}\right) \sqrt{2 T \ln \frac{1}{\delta}}+2 \operatorname{sp}\left(v^{*}\right)
$$

Proof. By Bellman equation for the discounted problem, we have $Q^{*}\left(s_{t}, a_{t}\right)-\gamma V^{*}\left(s_{t}\right)-r\left(s_{t}, a_{t}\right)=$ $\gamma\left(\mathbb{E}_{s^{\prime} \sim p\left(\cdot \mid s_{t}, a_{t}\right)}\left[V^{*}\left(s^{\prime}\right)\right]-V^{*}\left(s_{t}\right)\right) .$ Adding and subtracting $V^{*}\left(s_{t+1}\right)$ and summing over $t$ we will get
$$
\sum_{t=1}^{T}\left(Q^{*}\left(s_{t}, a_{t}\right)-\gamma V^{*}\left(s_{t}\right)-r\left(s_{t}, a_{t}\right)\right)=\gamma \sum_{t=1}^{T}\left(\mathbb{E}_{s^{\prime} \sim p\left(\cdot \mid s_{t}, a_{t}\right)}\left[V^{*}\left(s^{\prime}\right)\right]-V^{*}\left(s_{t+1}\right)\right)+\gamma \sum_{t=1}^{T}\left(V^{*}\left(s_{t+1}\right)-V^{*}\left(s_{t}\right)\right)
$$
The summands of the first term on the right hand side constitute a martingale difference sequence. Thus, by Azuma's inequality (Lemma 11) and the fact that $\operatorname{sp}\left(V^{*}\right) \leq 2 \operatorname{sp}\left(v^{*}\right)$ (Lemma 2), this term is upper bounded by $2 \gamma \operatorname{sp}\left(v^{*}\right) \sqrt{2 T \ln \frac{1}{\delta}}$, with probability at least $1-\delta .$ The second term is equal to $\gamma\left(V^{*}\left(s_{T+1}\right)-V^{*}\left(s_{1}\right)\right)$ which is upper bounded by $2 \gamma \operatorname{sp}\left(v^{*}\right)$. Recalling $\gamma<1$ completes the proof.
