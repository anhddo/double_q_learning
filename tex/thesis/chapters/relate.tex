\section{Jin 2018}
Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn 7 . 22 ]. The theoretical question of "whether model-free algorithms can be made sample efficient" is one of the most fundamental questions in $\mathrm{RL},$ and remains unsolved even in the basic scenario with finitely many states and actions.

We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret $\tilde{\mathcal{O}}\left(\sqrt{H^{3} S A T}\right),$ where $S$ and $A$ are the numbers of states and actions, $H$ is the number of steps per episode, and $T$ is the total number of steps. This sample efficiency matches the optimal regret that can be achieved by any model-based approach, up to a single $\sqrt{H}$ factor. To the best of our knowledge, this is the first analysis in the model-free setting that establishes $\sqrt{T}$ regret without requiring access to a "simulator."
\section{Jin 2019}
\subsection{ Linear Markov decision processes}
We focus on a setting of a linear Markov decision process, where the transition kernels and the reward function are assumed to be linear. This assumption implies that the action-value function is linear, as we will show. Note that this is $n o t$ the same as the assumption that the policy is a linear function -an assumption that has been the focus of much of the literature. Rather, it is akin to a statistical modeling assumption, in which we make assumptions about how data are generated and then study various estimators. Formally, we make the following definition.
Assumption A (Linear MDP [12,29] ). $\operatorname{MDP}(\mathcal{S}, \mathcal{A}, \mathrm{H}, \mathbb{P}, \mathrm{r})$ is a linear $M D P$ with a feature $\operatorname{map} \phi: \mathcal{S} \times$
$\mathcal{A} \rightarrow \mathbb{R}^{d},$ if for any $h \in[H],$ there exist $d$ unknown $($ signed $)$ measures $\mu_{h}=\left(\mu_{h}^{(1)}, \ldots, \mu_{h}^{(d)}\right)$ over $\mathcal{S}$ and an unknown vector $\theta_{h} \in \mathbb{R}^{d},$ such that for any $(x, a) \in \mathcal{S} \times \mathcal{A},$ we have
$$
\mathbb{P}_{h}(\cdot \mid x, a)=\left\langle\boldsymbol{\phi}(x, a), \boldsymbol{\mu}_{h}(\cdot)\right\rangle, \quad r_{h}(x, a)=\left\langle\boldsymbol{\phi}(x, a), \boldsymbol{\theta}_{h}\right\rangle
$$
Without loss of generality, we assume $\|\phi(x, a)\| \leq 1$ for all $(x, a) \in \mathcal{S} \times \mathcal{A},$ and $\max \left\{\left\|\mu_{h}(\mathcal{S})\right\|,\left\|\theta_{h}\right\|\right\} \leq$
$\sqrt{d}$ for all $h \in[H]$
By definition, in a linear MDP, both the Markov transition model and the reward functions are linear in a feature mapping $\phi .$ We remark that despite being linear, the Markov transition model $\mathbb{P}_{h}(\cdot \mid x, a)$ can still have infinite degrees of freedom as the measure $\mu_{h}$ is unknown. This is a key difference from the linear quadratic regulator [1,18,4,13,15] or the recent work of Yang and Wang [50] , whose transition models are completely specified by a finite-dimensional matrix such that the degrees of freedom are bounded.

Recall that we assume the reward functions are bounded in [0,1] , which implies that the value functions are bounded in $[0, H] .$ Our choice of normalization conditions in Assumption ? implies that the following concrete examples serve as special cases of a linear MDP.
\subsection{mechanisms}
In this section, we overview several of the key ideas behind the regret bound in Theorem $\frac{1}{3.1}$. We defer the
In Section 3 , we mentioned that the LSVI algorithm is motivated from the Bellman optimality equation Eq. (?). It remains to verify that line 5 in Algorithm $\square$ indeed well approximates the Bellman optimality equation, which turns out to require not only the linear MDP structure but also hinges on several other facts. To simplify our presentation, in this section we treat the regularization parameter $\lambda$ loosely as being sufficiently small so that $\Lambda_{h}^{-1} \sum_{\tau=1}^{k-1} \phi\left(x_{h}^{\tau}, a_{h}^{\tau}\right) \phi\left(x_{h}^{\tau}, a_{h}^{\tau}\right)^{\top} \approx$ I. We will focus in this section on a fixed
episode $k,$ and drop the dependency of parameters and value functions on $k$ when it is clear from the context. Now, ignoring the UCB bonus, the least-squares solution (line $[5$ ) gives the following estimate of the actionvalue function:
$$
Q_{h}(x, a) \approx \phi(x, a)^{\top} \mathbf{w}_{h}=\phi(x, a)^{\top} \Lambda_{h}^{-1} \sum_{\tau=1}^{k-1} \phi\left(x_{h}^{\tau}, a_{h}^{\tau}\right)\left[r_{h}\left(x_{h}^{\tau}, a_{h}^{\tau}\right)+V_{h+1}\left(x_{h+1}^{\tau}\right)\right]
$$
where $V_{h+1}(\cdot)=\max _{a \in \mathcal{A}} Q_{h+1}(\cdot, a) .$ Plugging in $r_{h}(\cdot, \cdot)=\phi(\cdot, \cdot)^{\top} \boldsymbol{\theta}_{h},$ we know the first term on the
of right-hand side approximates $\mathbb{P}_{h} V_{h+1}(x, a) .$ We thus define our empirical Markov transition measure as
$$
\widehat{\mathbb{P}}_{h}(\cdot \mid x, a):=\boldsymbol{\phi}(x, a)^{\top} \Lambda_{h}^{-1} \sum_{\tau=1}^{k-1} \boldsymbol{\phi}\left(x_{h}^{\tau}, a_{h}^{\tau}\right) \delta\left(\cdot, x_{h+1}^{\tau}\right)
$$
where the $\delta$ -measure $\delta(\cdot, x)$ puts an atom on element $x$. It remains to verify that $\widehat{\mathbb{P}}_{h} V_{h+1}(x, a) \approx \mathbb{P}_{h} V_{h+1}(x, a)$ To establish this, we use a measure $\overline{\mathbb{P}}_{h}$ to bridge these two quantities:
$$
\overline{\mathbb{P}}_{h}(\cdot \mid x, a):=\boldsymbol{\phi}(x, a)^{\top} \Lambda_{h}^{-1} \sum_{\tau=1}^{k-1} \boldsymbol{\phi}\left(x_{h}^{\tau}, a_{h}^{\tau}\right) \mathbb{P}_{h}\left(\cdot \mid x_{h}^{\tau}, a_{h}^{\tau}\right)
$$

\section{Other}
\begin{table}
\begin{tabular}{ |c|c|c|c| } 
 \hline
    &  \textbf{Algorithm} & \textbf{Regret} &  \textbf{Comment} \\ 
 \hline
 \multirow{4}{4em}{Model-based}   & REGAL & $\widetilde{\mathcal{O}}\left(\operatorname{sp}\left(v^{*}\right) \sqrt{S A T}\right)$& No efficient implementation \\ 
    & UCRL2 & $\widetilde{\mathcal{O}}(D S \sqrt{A T})$& No efficient implementation \\ 
    & PSRL & $\tilde{O}\left(\operatorname{sp}\left(v^{*}\right) S \sqrt{A T}\right)$& No efficient implementation \\ 
 \hline
\end{tabular}
    \caption{Regret comparison for RL algorithms}
    \label{table:regretcomp}
\end{table}
Undiscounted MDPs. Most no-regret algorithms for infinite-horizon undiscounted MDPs focus on tabular representations, and apply optimism to a model of the transition dynamics (Bartlett \& Tewari, 2009 , Fruit et al., 2018 , Jaksch et al., 2010 , Jian et al., 2019 , Ouyang et al., 2017 , Talebi \& Maillard,
2018). For a weakly-communicating MDP with $S$ states, $A$ actions, and diameter $D$, these algorithms achieve the minimax lower bound of $\mathcal{O}(\sqrt{D S A T})$ (Jaksch et al., 2010 ) with high probability up to constant factors (typically $\sqrt{D S}$ ). The downside of model-based algorithms is that they are memory intensive in large-scale MDPs, as they require $S^{2} A$ storage, and as well as being difficult to extend to continuous-valued states.

Among model-free algorithms for tabular MDPs,\cite{wei2020modelfree} show that optimistic Q-learning achieves $\mathcal{O}\left(\operatorname{sp}\left(V_{*}\right)(S A)^{1 / 3} T^{2 / 3}\right)$ regret in weakly-communicating MDPs, where $\operatorname{sp}\left(V_{*}\right)$ is the span of the optimal state-value function (upper bounded by the diameter $D$ ). In the case of uniformly ergodic MDPs, they show a bound of $\mathcal{O}\left(\sqrt{t_{\operatorname{mix}}^{3} \rho A T}\right)$ on expected regret, where $t_{\operatorname{mix}}$ is the mixing time and $\rho$ is the stationary distribution mismatch coefficient. Our optimistic value iteration is a non-trivial generalization of their Q-learning approach to a setting with function approximation.
In the model-free setting with function approximation, the Politex algorithm (Abbasi-Yadkori et al., 2019 , a variant of approximate policy iteration, achieves $\mathcal{O}\left(d^{1 / 2} T^{3 / 4}\right),$ where $d$ is the size of the compressed state-action space (i.e. number of features). The AAPI algorithm of Hao et al., 2020 achieves regret $\mathcal{O}\left(T^{2 / 3}\right)$ using a version of POLITEX with an adaptive learning rate. However, these algorithms do not explicitly tackle exploration, and instead make additional assumptions - namely, that the MDP is ergodic, and that all policies are sufficiently exploratory so that the value functions can be estimated with small error from on-policy data.

Episodic MDPs. Optimistic value-based algorithms have been more popular in episodic MDPs. In the tabular case with horizon $H,$ Jin et al., 2018 show an $\mathcal{O}\left(\sqrt{H^{3} S A T}\right)$ regret bound for Q-learning with UCB-style exploration. With function approximation, Cai et al., 2019 , Jin et al., 2019 , L. F. Yang and Wang, 2019 show an $\mathcal{O}\left(\sqrt{d^{3} H^{3} T}\right)$ regret bound for an optimistic version of least-squares value iteration in linear MDPs. The RLSVI algorithm Osband et al., 2016 performs exploration in the value function parameter space, but its worse-case regret bound of $\mathcal{O}\left(\sqrt{H^{5} S^{3} A T}\right)$ holds only in the tabular setting (Russo, 2019 ).

Lemma 2 (Restated). Let $V^{*}$ be the optimal value function in the discounted MDP with discount factor $\gamma$ and $v^{*}$ be the optimal value function in the undiscounted MDP. Then,
1. $\left|J^{*}-(1-\gamma) V^{*}(s)\right| \leq(1-\gamma) \operatorname{sp}\left(v^{*}\right), \forall s \in \mathcal{S}$
2. $\operatorname{sp}\left(V^{*}\right) \leq 2 \operatorname{sp}\left(v^{*}\right)$
Let $\pi^{*}$ and $\pi_{\gamma}$ be the optimal policy under undiscounted and discounted settings, respectively. By Bellman's equation, we have
$$
v^{*}(s)=r\left(s, \pi^{*}(s)\right)-J^{*}+\mathbb{E}_{s^{\prime} \sim p\left(\cdot \mid s, \pi^{*}(s)\right)} v^{*}\left(s^{\prime}\right)
$$
Consider a state sequence $s_{1}, s_{2}, \cdots$ generated by $\pi^{*} .$ Then, by sub-optimality of $\pi^{*}$ for the discounted setting, we have
$$
\begin{aligned}
V^{*}\left(s_{1}\right) & \geq \mathbb{E}\left[\sum_{t=1}^{\infty} \gamma^{t-1} r\left(s_{t}, \pi^{*}\left(s_{t}\right)\right) \mid s_{1}\right] \\
&=\mathbb{E}\left[\sum_{t=1}^{\infty} \gamma^{t-1}\left(J^{*}+v^{*}\left(s_{t}\right)-v^{*}\left(s_{t+1}\right)\right) \mid s_{1}\right] \\
&=\frac{J^{*}}{1-\gamma}+v^{*}\left(s_{1}\right)-\mathbb{E}\left[\sum_{t=2}^{\infty}\left(\gamma^{t-2}-\gamma^{t-1}\right) v^{*}\left(s_{t}\right) \mid s_{1}\right] \\
& \geq \frac{J^{*}}{1-\gamma}+\min _{s} v^{*}(s)-\max _{s} v^{*}(s) \sum_{t=2}^{\infty}\left(\gamma^{t-2}-\gamma^{t-1}\right) \\
&=\frac{J^{*}}{1-\gamma}-\operatorname{sp}\left(v^{*}\right)
\end{aligned}
$$

where the first equality is by the Bellman equation for the undiscounted setting.
Similarly, for the other direction, let $s_{1}, s_{2}, \cdots$ be generated by $\pi_{\gamma}$. We have
$$
\begin{aligned}
V^{*}\left(s_{1}\right) &=\mathbb{E}\left[\sum_{t=1}^{\infty} \gamma^{t-1} r\left(s_{t}, \pi_{\gamma}\left(s_{t}\right)\right) \mid s_{1}\right] \\
& \leq \mathbb{E}\left[\sum_{t=1}^{\infty} \gamma^{t-1}\left(J^{*}+v^{*}\left(s_{t}\right)-v^{*}\left(s_{t+1}\right)\right) \mid s_{1}\right] \\
&=\frac{J^{*}}{1-\gamma}+v^{*}\left(s_{1}\right)-\mathbb{E}\left[\sum_{t=2}^{\infty}\left(\gamma^{t-2}-\gamma^{t-1}\right) v^{*}\left(s_{t}\right) \mid s_{1}\right] \\
& \leq \frac{J^{*}}{1-\gamma}+\max _{s} v^{*}(s)-\min _{s} v^{*}(s) \sum_{t=2}^{\infty}\left(\gamma^{t-2}-\gamma^{t-1}\right) \\
&=\frac{J^{*}}{1-\gamma}+\operatorname{sp}\left(v^{*}\right)
\end{aligned}
$$
where the first inequality is by sub-optimality of $\pi_{\gamma}$ for the undiscounted setting.
2. Using previous part, for any $s_{1}, s_{2} \in \mathcal{S},$ we have
$$
\left|V^{*}\left(s_{1}\right)-V^{*}\left(s_{2}\right)\right| \leq\left|V^{*}\left(s_{1}\right)-\frac{J^{*}}{1-\gamma}\right|+\left|V^{*}\left(s_{2}\right)-\frac{J^{*}}{1-\gamma}\right| \leq 2 \operatorname{sp}\left(v^{*}\right)
$$
Thus, $\operatorname{sp}\left(V^{*}\right) \leq 2 \operatorname{sp}\left(v^{*}\right)$


Lemma 4. With probability at least $1-\delta$,
$$
\sum_{t=1}^{T}\left(Q^{*}\left(s_{t}, a_{t}\right)-\gamma V^{*}\left(s_{t}\right)-r\left(s_{t}, a_{t}\right)\right) \leq 2 \operatorname{sp}\left(v^{*}\right) \sqrt{2 T \ln \frac{1}{\delta}}+2 \operatorname{sp}\left(v^{*}\right)
$$

Proof. By Bellman equation for the discounted problem, we have $Q^{*}\left(s_{t}, a_{t}\right)-\gamma V^{*}\left(s_{t}\right)-r\left(s_{t}, a_{t}\right)=$ $\gamma\left(\mathbb{E}_{s^{\prime} \sim p\left(\cdot \mid s_{t}, a_{t}\right)}\left[V^{*}\left(s^{\prime}\right)\right]-V^{*}\left(s_{t}\right)\right) .$ Adding and subtracting $V^{*}\left(s_{t+1}\right)$ and summing over $t$ we will get
$$
\sum_{t=1}^{T}\left(Q^{*}\left(s_{t}, a_{t}\right)-\gamma V^{*}\left(s_{t}\right)-r\left(s_{t}, a_{t}\right)\right)=\gamma \sum_{t=1}^{T}\left(\mathbb{E}_{s^{\prime} \sim p\left(\cdot \mid s_{t}, a_{t}\right)}\left[V^{*}\left(s^{\prime}\right)\right]-V^{*}\left(s_{t+1}\right)\right)+\gamma \sum_{t=1}^{T}\left(V^{*}\left(s_{t+1}\right)-V^{*}\left(s_{t}\right)\right)
$$
The summands of the first term on the right hand side constitute a martingale difference sequence. Thus, by Azuma's inequality (Lemma 11) and the fact that $\operatorname{sp}\left(V^{*}\right) \leq 2 \operatorname{sp}\left(v^{*}\right)$ (Lemma 2), this term is upper bounded by $2 \gamma \operatorname{sp}\left(v^{*}\right) \sqrt{2 T \ln \frac{1}{\delta}}$, with probability at least $1-\delta .$ The second term is equal to $\gamma\left(V^{*}\left(s_{T+1}\right)-V^{*}\left(s_{1}\right)\right)$ which is upper bounded by $2 \gamma \operatorname{sp}\left(v^{*}\right)$. Recalling $\gamma<1$ completes the proof.
